{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYQiprE0pcVD"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <strong><font size=\"6\">Deep Learning 2025 Project</font></strong><br><br>\n",
        "  Podavini Luca 257844<br>\n",
        "  Richichi Andrea 257850<br>\n",
        "  Sorrentino Francesco 256151\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIkZmp9C84Tj"
      },
      "source": [
        "# Introduction\n",
        "The aim of this project is to implement a **PEFT** (Parameter Efficient Fine Tuning) technique for CLIP and to find a strategy to improve it.\n",
        "The chosen task used to evaluate the model is a base-to-novel classification task on the Flowers102 dataset.\n",
        "The work is divided into:\n",
        "1. Evaluation of Zero-Shot CLIP\n",
        "2. Evaluation of CoCoOp\n",
        "3. Evaluation of CLIP-LoRA\n",
        "4. Evaluation of DISEF (an improvement on CLIP-LoRA)\n",
        "5. Evaluation of our improved DISEF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQNgqrN-14gT"
      },
      "source": [
        "## Import modules\n",
        "This project requires different modules to work properly some of which are not pre-installed in colab. We will use the following modules:\n",
        "- torch: torch library.\n",
        "- torchvision: library containing data.\n",
        "- clip: containing the pretrained model.\n",
        "- tqdm: progress bar.\n",
        "- matplotlib: plotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UzXtFjhh7iOS",
        "outputId": "f04dce6d-3106-436e-8b76-503e69992dd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai_clip in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from openai_clip) (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from openai_clip) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai_clip) (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->openai_clip) (0.2.13)\n"
          ]
        }
      ],
      "source": [
        "# Install not pre-installed modules\n",
        "%pip install openai_clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "QtqdSOr8qqOn"
      },
      "outputs": [],
      "source": [
        "# Importing modules\n",
        "\n",
        "# Modules for the whole project\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import clip\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Modules for CoCoOp\n",
        "from collections import OrderedDict\n",
        "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "\n",
        "# Modules for LoRA\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Modules for DISEF\n",
        "from PIL import Image\n",
        "\n",
        "# Modules for our DISEF\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMx2sdWdB1ak"
      },
      "source": [
        "## Define constants\n",
        "Here we define constants and parameters to run the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "PGc5F2xuDF5c"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "# Device where code is run\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# CLIP Backbone\n",
        "CLIP_BACKBONE = \"ViT-B/16\"\n",
        "\n",
        "# Classnames in the dataset, hardcoded for use later\n",
        "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
        "# Prompt default format\n",
        "PROMPT_FORMAT = \"a photo of a {}, a type of flower.\"\n",
        "\n",
        "# Parameters to decide what evaluations must be run\n",
        "RUN_ZERO_SHOT = False\n",
        "RUN_COCOOP = False\n",
        "RUN_LORA = False\n",
        "RUN_DISEF = False\n",
        "RUN_OUR_DISEF = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG7xz_Rx3NkS"
      },
      "source": [
        "## Load Dataset\n",
        "Collecting and preprocessing data from torchvision.\n",
        "We will use the Flowers102 dataset.\n",
        "\n",
        "The classes are split into base and novel ones by putting half the classes into base and the other half into novel. This only simulates a real application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5VoyArd-O1ZJ"
      },
      "outputs": [],
      "source": [
        "def get_data(data_dir=\"./data\", transform=None):\n",
        "    \"\"\"Load Flowers102 train, validation and test sets.\n",
        "    Args:\n",
        "        data_dir (str): Directory where the dataset will be stored.\n",
        "        transform (torch.Compose)\n",
        "    Returns:\n",
        "        tuple: A tuple containing the train, validation, and test sets.\n",
        "    \"\"\"\n",
        "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
        "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
        "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
        "    return train, val, test\n",
        "\n",
        "def base_novel_categories(dataset):\n",
        "  \"\"\"Split dataset classes into base and novel classes.\n",
        "  Args:\n",
        "    dataset (list): Dataset to split into base and novel classes.\n",
        "  Returns:\n",
        "    tuple: A tuple containing base and novel classes\n",
        "  \"\"\"\n",
        "  # Set returns the unique set of all dataset classes\n",
        "  all_classes = set(dataset._labels)\n",
        "  num_classes = len(all_classes)\n",
        "\n",
        "  # Generate base and novel category lists\n",
        "  base_classes = list(range(num_classes))[:num_classes//2]\n",
        "  novel_classes = list(range(num_classes))[num_classes//2:]\n",
        "  return base_classes, novel_classes\n",
        "\n",
        "\n",
        "def split_data(dataset, base_classes):\n",
        "  \"\"\"Split sample given base classes.\n",
        "  Args:\n",
        "    dataset (list): list of samples.\n",
        "    base_classes (list): list of base classes.\n",
        "  Returns:\n",
        "    tuple: Tuple containing base and novel datasets.\n",
        "  \"\"\"\n",
        "  # List to store sample idx\n",
        "  base_categories_samples = []\n",
        "  novel_categories_samples = []\n",
        "\n",
        "  # Set of base classes to compute the test below in O(1)\n",
        "  base_set = set(base_classes)\n",
        "\n",
        "  # Iterate and get sample idx\n",
        "  for sample_id, label in enumerate(dataset._labels):\n",
        "    if label in base_set:\n",
        "      base_categories_samples.append(sample_id)\n",
        "    else:\n",
        "      novel_categories_samples.append(sample_id)\n",
        "\n",
        "  # Create the dataset subsets using Subset\n",
        "  base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
        "  novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
        "  return base_dataset, novel_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P60x1AEE-Hs"
      },
      "source": [
        "Now we can write a function to load data enabling or not preprocessing with CLIP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "z77ElkswECZX"
      },
      "outputs": [],
      "source": [
        "def get_dataset(do_preprocess=True):\n",
        "  \"\"\"Load Flowers102 datasets\n",
        "  Args:\n",
        "    preprocess (bool): enable or not preprocessing using CLIP.\n",
        "  Returns:\n",
        "    tuple: A tuple containing the train, validation, test sets, base classes and novel classes.\n",
        "  \"\"\"\n",
        "  if do_preprocess:\n",
        "    # Load the CLIP preprocess\n",
        "    _, clip_preprocess = clip.load(CLIP_BACKBONE, device=DEVICE)\n",
        "\n",
        "  # Get the three datasets using the CLIP preprocess\n",
        "  train_set, val_set, test_set = get_data(transform=clip_preprocess)\n",
        "\n",
        "  # Split classes into base and novel\n",
        "  base_classes, novel_classes = base_novel_categories(train_set)\n",
        "\n",
        "  # Split the three datasets\n",
        "  train_base, _ = split_data(train_set, base_classes)\n",
        "  val_base, _ = split_data(val_set, base_classes)\n",
        "  test_base, test_novel = split_data(test_set, base_classes)\n",
        "\n",
        "  return train_base, val_base, test_base, test_novel, base_classes, novel_classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvKtllMsX7YL"
      },
      "source": [
        "Now data can be loaded and preprocessed to get our datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0VbV2KGJJgX"
      },
      "source": [
        "Before evaluating the model, let's study the number of samples and class distribution.\n",
        "\n",
        "<img src=\"sample_counts_per_dataset.png\" width=\"600\"/>\n",
        "\n",
        "We have 510 samples for both training set and validation set.\n",
        "Test base contains 2473 samples while test novel contains 3676 samples.\n",
        "\n",
        "<img src=\"class_distribution_Train_Base\" width=\"600\"/>\n",
        "<img src=\"class_distribution_Val_Base\" width=\"600\"/>\n",
        "\n",
        "*train_base* and *val_base* contain 10 shots for every base class (51 classes * 10 shots).\n",
        "\n",
        "<img src=\"class_distribution_Test_Base\" width=\"600\"/>\n",
        "\n",
        "*test_base* contains a non-uniform distribution of samples through classes with the most amount of samples (> 200) on petunia and wallflower classes.\n",
        "\n",
        "<img src=\"class_distribution_Test_Novel\" width=\"600\"/>\n",
        "\n",
        "*test_novel* contains a non-uniform distribution of samples through classes with the most amount of samples (> 200) on passion flower."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWtHrmcqYQJn"
      },
      "source": [
        "# Evaluation of Zero-Shot CLIP\n",
        "Zero-Shot CLIP must be evaluated before applying any technique in order to have a baseline performance to improve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m873CjrhGmWz"
      },
      "source": [
        "## Test functions\n",
        "\n",
        "First a test function is define to be re-used later in the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "xZQHqu8xGwuD"
      },
      "outputs": [],
      "source": [
        "def harmonic_mean(base_accuracy, novel_accuracy):\n",
        "  \"\"\"Compute harmonic mean\n",
        "  Args:\n",
        "    base_accuracy (float): accuracy score on base classes.\n",
        "    novel_accuracy (float): accuracy score on novel classes.\n",
        "  Returns:\n",
        "    float: harmonic mean.\n",
        "  \"\"\"\n",
        "  numerator = 2\n",
        "  denominator = 1 / base_accuracy + 1 / novel_accuracy\n",
        "  hm = numerator / denominator\n",
        "  return hm\n",
        "\n",
        "def clip_test(model, loader, categories, device, label=\"\"):\n",
        "  \"\"\"Test function for CLIP model.\n",
        "  Args:\n",
        "    model (torch.nn): clip pretrained model to use.\n",
        "    loader (DataLoader): dataloader for evaluation.\n",
        "    categories (list): either base or novel idxs.\n",
        "    device (str): device where to put data.\n",
        "    label (str): label for evaluation loop.\n",
        "  Returns:\n",
        "    float: accuracy score.\n",
        "  \"\"\"\n",
        "  # Set model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # Dictionary for remapping labels label -> into contiguous set\n",
        "  contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
        "\n",
        "  # Apply the standard CLIP template used for oxford flowers to all categories and immediately tokenize each sentence\n",
        "  text_inputs = clip.tokenize([PROMPT_FORMAT.format(CLASS_NAMES[c]) for c in categories]).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # Encode text features for all classes\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "    # Normalize them (standard pratice with CLIP)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Variable to store number of correct predictions\n",
        "    correct_predictions = 0\n",
        "    # Iterate through batches\n",
        "    for image, target in tqdm(loader, desc=label):\n",
        "      # Map categories to contiguous to get correct predictions\n",
        "      target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
        "\n",
        "      image, target = image.to(device), target.to(device)\n",
        "\n",
        "      # Encode image features\n",
        "      image_features = model.encode_image(image)\n",
        "      # Normalize image features\n",
        "      image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "      # Cosine similarity between image and text features and keep the argmax for every row (every image)\n",
        "      predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
        "      # Check which are correct, and sum them (False == 0, True == 1)\n",
        "      correct_predictions += (predicted_class == target).sum().item()\n",
        "  # Compute the accuracy\n",
        "  accuracy = correct_predictions / len(loader.dataset)\n",
        "  return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhYREXe7XgVw"
      },
      "source": [
        "After having defined a general test function for CLIP we can run an evaluation function for zero-shot model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "LFVgL-0nXuQQ"
      },
      "outputs": [],
      "source": [
        "def zero_shot_eval():\n",
        "  \"\"\"Function to run zero_shot evaluation.\"\"\"\n",
        "  # Load the model\n",
        "  clip_model, _ = clip.load(CLIP_BACKBONE, device=DEVICE)\n",
        "\n",
        "\n",
        "  # Get the datasets\n",
        "  _, _, test_base, test_novel, base_classes, novel_classes = get_dataset(do_preprocess=True)\n",
        "\n",
        "  # Batch size\n",
        "  TEST_BATCH = 128\n",
        "\n",
        "  # Get loaders\n",
        "  test_base_loader = torch.utils.data.DataLoader(test_base, batch_size=TEST_BATCH, shuffle=False, num_workers=2)\n",
        "  test_novel_loader = torch.utils.data.DataLoader(test_novel, batch_size=TEST_BATCH, shuffle=False, num_workers=2)\n",
        "\n",
        "  # Evaluate base and novel\n",
        "  base_accuracy = clip_test(clip_model, test_base_loader, base_classes, DEVICE, label=\"🧠 Zero-shot evaluation on Base\")\n",
        "  novel_accuracy = clip_test(clip_model, test_novel_loader, novel_classes, DEVICE, label=\"🧠 Zero-shot evaluation on Novel\")\n",
        "\n",
        "  # Show results\n",
        "  print() # For separating from progress bars\n",
        "  print(\"=\"*20,\"Evaluating Zero-Shot\",\"=\"*20)\n",
        "  print(f\"🔍 Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
        "  print(f\"🔍 Novel classes accuracy: {novel_accuracy*100:.2f}%\")\n",
        "  print(f\"🔍 Harmonic Mean: {harmonic_mean(base_accuracy, novel_accuracy)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Xbk5u7wCGBsF"
      },
      "outputs": [],
      "source": [
        "if RUN_ZERO_SHOT:\n",
        "  zero_shot_eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_GwI2Jed-YK"
      },
      "source": [
        "The baseline obtained with zero-shot evaluation is:\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "| Model      | Base (↑) | Novel (↑) | Harmonic Mean |\n",
        "|------------|----------|-----------|---------------|\n",
        "| Zero-Shot  | 71.33%   | 78.24%    | 74.62%        |\n",
        "\n",
        "<div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kts9WcP02Scm"
      },
      "source": [
        "# Evaluation of CoCoOp\n",
        "The first technique applied to CLIP to have a comparison later on is CoCoOp.\n",
        "This technique introduces the work into the field of prompt tuning by learning textual prompts that improve the zero-shot performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cBnvvePy9T6"
      },
      "source": [
        "Create the `TextEncoder` class to encode dynamic prompts with tokenized prompts given."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "UoKry-Ksp3AF"
      },
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "  \"\"\"Encode dynamic prompts with tokenized given prompts.\"\"\"\n",
        "  def __init__(self, clip_model):\n",
        "    \"\"\"Init the module.\n",
        "    Args:\n",
        "      clip_model (torch.nn): clip pretrained model to use.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    # Save into the module clip modules\n",
        "    self.text_encoder = clip_model.transformer\n",
        "    self.positional_embedding = clip_model.positional_embedding\n",
        "    self.layer_norm = clip_model.ln_final\n",
        "    self.proj = clip_model.text_projection\n",
        "\n",
        "  def forward(self, embedded_tokens, tokens_ids):\n",
        "    \"\"\"Forward pass.\n",
        "    Args:\n",
        "      embedded_tokens (Tensor): Input token embeddings.\n",
        "      tokens_ids (Tensor): Tokenizer prompt IDs.\n",
        "    \"\"\"\n",
        "    # Apply positional embeddings\n",
        "    output = embedded_tokens + self.positional_embedding\n",
        "\n",
        "    # Rearrange dimension for transformer input\n",
        "    output = output.permute(1, 0, 2)  # [batch_size, n_ctx, transformer.width] -> [n_ctx, batch_size, transformer.width]\n",
        "    output = self.text_encoder(output)\n",
        "    # Go back to original dimensions\n",
        "    output = output.permute(1, 0, 2)  # [n_ctx, batch_size, transformer.width] -> [batch_size, n_ctx, transformer.width]\n",
        "    # Apply layer norm\n",
        "    output = self.layer_norm(output)\n",
        "\n",
        "    # Select features corresponding to eot tokens\n",
        "    eot_ids = tokens_ids.argmax(dim=-1)\n",
        "    output = output[torch.arange(output.shape[0]), eot_ids] @ self.proj\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7j0_Ih6zdia"
      },
      "source": [
        "The `PromptLearner` class then handles the prompt creation given an inital context or a random context lenght."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "BlBcAm_Gp2-F"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PromptLearner(nn.Module):\n",
        "  \"\"\"Module that learns ctx vectors to adapt to visual features for each class.\"\"\"\n",
        "  def __init__(self, n_ctx, ctx_init, classnames, clip_model):\n",
        "    \"\"\"Initialize PromptLearner module.\n",
        "    Args:\n",
        "      n_ctx (int): Number of ctx tokens to learn.\n",
        "      ctx_init (string): Initialization string for context.\n",
        "      classnames (list): class names list.\n",
        "      clip_model (nn.Module): pretrained CLIP for token embedding.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    # Save number of classes and number of ctx tokens\n",
        "    self.n_cls = len(classnames)\n",
        "    self.n_ctx = n_ctx\n",
        "\n",
        "    # Get dimensions from CLIP\n",
        "    # Context embedding dimension\n",
        "    ctx_dim = clip_model.ln_final.weight.shape[0]\n",
        "    # Visual encoder output dimension\n",
        "    vis_dim = clip_model.visual.output_dim\n",
        "\n",
        "    # Get CLIP device\n",
        "    device = clip_model.token_embedding.weight.device\n",
        "\n",
        "    # Use given words to initialize context vectors\n",
        "    ctx_init = ctx_init.replace(\"_\", \" \")\n",
        "    self.n_ctx = len(ctx_init.split(\" \"))\n",
        "    prompt = clip.tokenize(ctx_init).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        embedding = clip_model.token_embedding(prompt) # Convert token to embedding\n",
        "\n",
        "    # Get only context tokens\n",
        "    ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
        "    prompt_prefix = ctx_init\n",
        "\n",
        "    # Register learnable context parameters\n",
        "    self.ctx = nn.Parameter(ctx_vectors)\n",
        "\n",
        "    # Meta-net to apply context based on image features\n",
        "    self.meta_net = nn.Sequential(OrderedDict([\n",
        "        (\"linear1\", nn.Linear(vis_dim, vis_dim // 16)),\n",
        "        (\"relu\", nn.ReLU(inplace=True)),\n",
        "        (\"linear2\", nn.Linear(vis_dim // 16, ctx_dim))\n",
        "    ]))\n",
        "\n",
        "    # Preprocess classnames and add them to the prompts\n",
        "    classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
        "    # Instantiate tokenizer to get lenght of classnames\n",
        "    _tokenizer = _Tokenizer()\n",
        "    self.name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
        "    prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
        "\n",
        "    # Tokenize prompts and get embeddings\n",
        "    tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(device)  # (n_cls, n_tkn)\n",
        "    with torch.no_grad():\n",
        "        embedding = clip_model.token_embedding(tokenized_prompts)\n",
        "\n",
        "    # Register unchanged parts of the embeddings\n",
        "    self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
        "    self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS\n",
        "\n",
        "    # Save tokenized_prompts\n",
        "    self.tokenized_prompts = tokenized_prompts  # torch.Tensor\n",
        "\n",
        "  def construct_prompts(self, ctx, prefix, suffix, label=None):\n",
        "    \"\"\"Construct final prompts by concatenating prefix, ctx and suffix tokens.\n",
        "    Args:\n",
        "      ctx (Tensor): context tokens.\n",
        "      prefix (Tensor): prefix tokens.\n",
        "      suffix (Tensor): suffix tokens.\n",
        "      label (Tensor): indeces to select specific class prompts.\n",
        "    \"\"\"\n",
        "    if label is not None:\n",
        "      # Class specific suffix and prefix if labels are given\n",
        "      prefix = prefix[label]\n",
        "      suffix = suffix[label]\n",
        "\n",
        "    prompts = torch.cat(\n",
        "      [prefix,\n",
        "      ctx,\n",
        "      suffix],\n",
        "      dim=1\n",
        "    )\n",
        "\n",
        "    return prompts\n",
        "\n",
        "  def forward(self, im_features):\n",
        "    \"\"\"Forward pass.\n",
        "    Args:\n",
        "      im_features (Tensor): image features.\n",
        "    Returns:\n",
        "      Tensor: final prompts.\n",
        "    \"\"\"\n",
        "    # Get fixed prefix and suffix token embeddings\n",
        "    prefix = self.token_prefix\n",
        "    suffix = self.token_suffix\n",
        "\n",
        "    # Using the input image features we generate bias to apply to ctx\n",
        "    bias = self.meta_net(im_features)\n",
        "    bias = bias.unsqueeze(1)\n",
        "    ctx = self.ctx.unsqueeze(0)\n",
        "    ctx_shifted = ctx + bias # Broadcast bias across context\n",
        "\n",
        "    # Build prompts for each sample in batch using adapted context\n",
        "    prompts = []\n",
        "    for ctx_shifted_i in ctx_shifted:\n",
        "      # Expands for all classes\n",
        "      ctx_i = ctx_shifted_i.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
        "      pts_i = self.construct_prompts(ctx_i, prefix, suffix)\n",
        "      prompts.append(pts_i)\n",
        "\n",
        "    # Stack prompts for all batch samples\n",
        "    prompts = torch.stack(prompts)\n",
        "\n",
        "    return prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUGFxyqVzqE-"
      },
      "source": [
        "The `CoCoOpCLIP` class is a wrapper around clip to handle a clean forward method that takes prompts from the `PromptLearner` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "e_opd3dep24l"
      },
      "outputs": [],
      "source": [
        "class CoCoOpCLIP(nn.Module):\n",
        "  \"\"\"A CLIP wrapper that adds prompt tuning and computes logits using dynamic text features.\"\"\"\n",
        "  def __init__(self, n_ctx, ctx_init, classnames, clip_model):\n",
        "    \"\"\"Initiate module with prompt learner and encoders.\n",
        "    Args:\n",
        "      n_ctx (int): number of ctx tokens for prompt learner.\n",
        "      ctx_init (string): context init string for prompt learner.\n",
        "      classnames (list): list of classnames for prompt learner.\n",
        "      clip_model (nn.Module): pretrained CLIP to wrap around.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    # Prompt learner generates dynamic prompts based on image features\n",
        "    self.prompt_learner = PromptLearner(n_ctx, ctx_init, classnames, clip_model)\n",
        "    # Save tokenized prompts from prompt learner\n",
        "    self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
        "    # Get visual encoder from CLIP\n",
        "    self.image_encoder = clip_model.visual\n",
        "    # Get wrapper around clip text encoder\n",
        "    self.text_encoder = TextEncoder(clip_model)\n",
        "    # Learnable scaling factor for logits from CLIP\n",
        "    self.logit_scale = clip_model.logit_scale\n",
        "\n",
        "  def forward(self, imgs):\n",
        "    \"\"\"Forward pass to compute logits between image and text prompts.\n",
        "    Args:\n",
        "      imgs: batch of input images.\n",
        "    Returns:\n",
        "      Tensor: similarity logits between image and text features.\n",
        "    \"\"\"\n",
        "    # Get tokenized prompts and logit scale\n",
        "    tokenized_prompts = self.tokenized_prompts\n",
        "    logit_scale = self.logit_scale.exp()\n",
        "\n",
        "    # Encode and normalize image features\n",
        "    image_features = self.image_encoder(imgs)\n",
        "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Generate instance-conditioned prompts using image features\n",
        "    prompts = self.prompt_learner(image_features)\n",
        "\n",
        "    logits = []\n",
        "    for pts_i, imf_i in zip(prompts, image_features):\n",
        "      # Compute similarity for each image in the batch\n",
        "      text_features = self.text_encoder(pts_i, tokenized_prompts)\n",
        "      text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "      l_i = logit_scale * imf_i @ text_features.t()\n",
        "      logits.append(l_i)\n",
        "\n",
        "    # Stack logits for all batch samples\n",
        "    logits = torch.stack(logits)\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOT-a9M4z2_K"
      },
      "source": [
        "New training loop and evaluation loop functions have to be created in order to handle the wrapper module. Since we are doing few-shot, in testing phase we need to pass also the base or novel class idxs and remap labels to contiguos values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "RT6tVOiQ7v72"
      },
      "outputs": [],
      "source": [
        "def cocoop_test(cocoop_model, loader, categories, device=\"cuda:0\", label=\"\"):\n",
        "  \"\"\"Test CoCoOp model.\n",
        "  Args:\n",
        "    cocoop_model (nn.Module): CoCoOp model to test.\n",
        "    loader (DataLoader): dataloader for evaluation.\n",
        "    categories (list): either base or novel idxs.\n",
        "    device (str): device where to put data.\n",
        "  \"\"\"\n",
        "  # Variables to compute performance score\n",
        "  samples = 0\n",
        "  cumulative_accuracy = 0\n",
        "\n",
        "  # Set the network to evaluation mode\n",
        "  cocoop_model.eval()\n",
        "\n",
        "  # Remap labels into a contiguous set starting from zero\n",
        "  contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for images, targets in tqdm(loader, desc=label):\n",
        "      # Map categories to the [0, 50], otherwise we will have wrong predictions\n",
        "      targets = torch.Tensor([contig_cat2idx[t.item()] for t in targets]).long()\n",
        "      images, targets = images.to(device), targets.to(device)\n",
        "\n",
        "      outputs = cocoop_model(images)\n",
        "\n",
        "      # Compute performance scores\n",
        "      batch_size = images.shape[0]\n",
        "      samples += batch_size\n",
        "\n",
        "      _, predicted = outputs.max(1)\n",
        "\n",
        "      # Compute accuracy\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "    accuracy = cumulative_accuracy / samples\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a81Ul0_s0QHC"
      },
      "source": [
        "For a more clean code, we can also create a training function that trains the model for a given number of epochs. Here we also implement early stopping with patience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "x0utnka27-BX"
      },
      "outputs": [],
      "source": [
        "def cocoop_train(cocoop_model, train_loader, val_loader, optimizer, loss_fun, scheduler, base_classes, num_epochs=5, patience=3, device=\"cuda:0\"):\n",
        "  \"\"\"Train CoCoOp.\n",
        "  Args:\n",
        "    cocoop_model (nn.Module): CoCoOp model to train.\n",
        "    train_loader (DataLoader): dataloader for training.\n",
        "    val_loader (DataLoader): dataloader for validation.\n",
        "    optimizer (torch.optim): optimizer to use.\n",
        "    loss_fun (torch.nn): cost function to use.\n",
        "    scheduler (torch.optim.lr_scheduler): scheduler to use.\n",
        "    base_classes (list): list of base classes for evauation.\n",
        "    num_epochs (int): number of epochs to train.\n",
        "    patience (int): patience for early stopping.\n",
        "    device (str): device to use.\n",
        "  \"\"\"\n",
        "  def train_step(cocoop_model, loader, optimizer, loss_fun, device=\"cuda:0\"):\n",
        "    \"\"\"Training step for CoCoOp.\n",
        "    Args:\n",
        "      cocoop_model (nn.Module): CoCoOp model to train.\n",
        "      loader (data): loader for training set.\n",
        "      optimizer (torch.optim): optimizer to use.\n",
        "      loss_fun (torch.nn): cost function to use.\n",
        "      device (str): device to use.\n",
        "    \"\"\"\n",
        "    # Variables to store values to compute loss and accuracy\n",
        "    samples = 0\n",
        "    cumulative_loss = 0\n",
        "    cumulative_accuracy = 0\n",
        "\n",
        "    # Set the model to training mode\n",
        "    cocoop_model.train()\n",
        "\n",
        "    # Iterate over the training set\n",
        "    for images, targets in loader:\n",
        "      images = images.to(device)\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      outputs = cocoop_model(images)\n",
        "      loss = loss_fun(outputs, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      batch_size = images.shape[0]\n",
        "      samples += batch_size\n",
        "      cumulative_loss += loss.item() * batch_size\n",
        "\n",
        "      # Get predictions\n",
        "      _, predicted = outputs.max(dim=1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "    loss = cumulative_loss / samples\n",
        "    accuracy = cumulative_accuracy / samples\n",
        "\n",
        "    return loss, accuracy\n",
        "\n",
        "  # Initialize vars before the train loop\n",
        "  best_val_acc = float('-inf')\n",
        "  best_model_state = None\n",
        "  curr_patience = patience\n",
        "  pb = tqdm(range(num_epochs))\n",
        "\n",
        "  for epoch in pb:\n",
        "    # Training step\n",
        "    train_loss, train_acc = train_step(cocoop_model, train_loader, optimizer, loss_fun, device)\n",
        "\n",
        "    # Evaluation step\n",
        "    val_acc = cocoop_test(cocoop_model, val_loader, base_classes, device)\n",
        "\n",
        "    # Show val accuracy at every iteration\n",
        "    pb.set_description(f\"Val Acc: {val_acc*100:.2f}%\")\n",
        "\n",
        "    # If we get better metric save the model\n",
        "    if val_acc > best_val_acc:\n",
        "      best_val_acc = val_acc\n",
        "      best_model_state = cocoop_model.prompt_learner.state_dict()\n",
        "      curr_patience = patience\n",
        "    else:\n",
        "      if curr_patience < 1:\n",
        "        break\n",
        "      else:\n",
        "        curr_patience = curr_patience - 1\n",
        "    # Step the scheduler if provided\n",
        "    if scheduler is not None:\n",
        "      scheduler.step()\n",
        "\n",
        "  # Load the best model before returning\n",
        "  if best_model_state is not None:\n",
        "    cocoop_model.prompt_learner.load_state_dict(best_model_state)\n",
        "  return cocoop_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCTyiZxS0cf-"
      },
      "source": [
        "The previous training function generates a log and since we want to plot it we can create a utility to show the training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "IkI-kYVmTwfc"
      },
      "outputs": [],
      "source": [
        "def save_model(model, model_name):\n",
        "  \"\"\"Save the model weights.\n",
        "  Args:\n",
        "    model (nn.Module): model to save.\n",
        "    model_name (str): name of the model to save.\n",
        "  \"\"\"\n",
        "  # Save directory location (if does not exist create it)\n",
        "  save_dir = os.path.join(\"bin\")\n",
        "  os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "  file_path = os.path.join(save_dir, f\"{model_name}.pt\")\n",
        "  torch.save(model.state_dict(), file_path)\n",
        "  print(f\"{model_name} weights saved to {file_path}.\")\n",
        "\n",
        "def load_model(model, model_name, device):\n",
        "  \"\"\"Load the model weights.\n",
        "  Args:\n",
        "    model (nn.Module): model where to load the weights.\n",
        "    model_name (str): name of the model to load.\n",
        "    device (str): device to use.\n",
        "  \"\"\"\n",
        "  file_path = os.path.join(\"bin\", f\"{model_name}.pt\")\n",
        "\n",
        "  if not os.path.isfile(file_path):\n",
        "    raise FileNotFoundError(f\"No weights file found at {file_path}\")\n",
        "\n",
        "  state_dict = torch.load(file_path, map_location=torch.device(device))\n",
        "  model.load_state_dict(state_dict)\n",
        "  print(f\"Model weights loaded from {file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEZYWhHD1T9K"
      },
      "source": [
        "After all of this we can create an evaluate CoCoOp function that loads the different models, trains on base training set and gives us accuracy after training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Rpf6Usrb20y5"
      },
      "outputs": [],
      "source": [
        "def cocoop_eval(do_train=True):\n",
        "  \"\"\"Function to run CoCoOp evaluation.\n",
        "  Args:\n",
        "    do_train (bool): train the model or load weights.\n",
        "  \"\"\"\n",
        "  # Load the model\n",
        "  clip_model, _ = clip.load(CLIP_BACKBONE, device=DEVICE)\n",
        "  clip_model = clip_model.float() # To avoid weight type errors\n",
        "\n",
        "  # Get the datasets\n",
        "  train_base, val_base, test_base, test_novel, base_classes, novel_classes = get_dataset(do_preprocess=True)\n",
        "\n",
        "  # Hyperparams\n",
        "  N_CTX = 4\n",
        "  CTX_INIT = \"a photo of a\"\n",
        "  TRAIN_BATCH = 1 # CoCoOp requires a batch of 1 in training to run\n",
        "  VAL_BATCH = 64\n",
        "  TEST_BATCH = 128\n",
        "  LR = 2e-3\n",
        "  NUM_EPOCHS = 10 # From CoCoOp paper\n",
        "  PATIENCE = 3\n",
        "\n",
        "  # Get base and novel classnames\n",
        "  base_classnames = [CLASS_NAMES[c] for c in base_classes]\n",
        "  novel_classnames = [CLASS_NAMES[c] for c in novel_classes]\n",
        "\n",
        "  # One wrapper for base classification and the other for novel by giving different classnames\n",
        "  base_model = CoCoOpCLIP(N_CTX, CTX_INIT, base_classnames, clip_model).to(DEVICE)\n",
        "  novel_model = CoCoOpCLIP(N_CTX, CTX_INIT, novel_classnames, clip_model).to(DEVICE)\n",
        "\n",
        "  # Freeze all CLIP params other than prompt learner\n",
        "  for name, param in base_model.named_parameters():\n",
        "      if \"prompt_learner\" not in name:\n",
        "          param.requires_grad_(False)\n",
        "  for name, param in novel_model.named_parameters():\n",
        "      if \"prompt_learner\" not in name:\n",
        "          param.requires_grad_(False)\n",
        "\n",
        "  # Sanity check\n",
        "  enabled = set()\n",
        "  for name, param in base_model.named_parameters():\n",
        "      if param.requires_grad:\n",
        "          enabled.add(name)\n",
        "  print(f\"Parameters to be updated: {enabled}\")\n",
        "\n",
        "  # Get loaders\n",
        "  train_loader = torch.utils.data.DataLoader(train_base, batch_size=TRAIN_BATCH, shuffle=True, num_workers=2)\n",
        "  val_loader = torch.utils.data.DataLoader(val_base, batch_size=VAL_BATCH, shuffle=False, num_workers=2)\n",
        "  test_base_loader = torch.utils.data.DataLoader(test_base, batch_size=TEST_BATCH, shuffle=False, num_workers=2)\n",
        "  test_novel_loader = torch.utils.data.DataLoader(test_novel, batch_size=TEST_BATCH, shuffle=False, num_workers=2)\n",
        "\n",
        "  # Get cost function\n",
        "  loss_fun = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  # Train\n",
        "  if(do_train):\n",
        "    optimizer = optim.AdamW([p for p in base_model.parameters() if p.requires_grad], lr=LR)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
        "\n",
        "    # Train\n",
        "    train_params = {\n",
        "        \"cocoop_model\": base_model,\n",
        "        \"train_loader\": train_loader,\n",
        "        \"val_loader\": val_loader,\n",
        "        \"optimizer\": optimizer,\n",
        "        \"loss_fun\": loss_fun,\n",
        "        \"scheduler\": scheduler,\n",
        "        \"base_classes\": base_classes,\n",
        "        \"num_epochs\": NUM_EPOCHS,\n",
        "        \"patience\": PATIENCE,\n",
        "        \"device\": DEVICE\n",
        "    }\n",
        "    base_model = cocoop_train(**train_params)\n",
        "    # Save weights\n",
        "    save_model(base_model.prompt_learner, model_name=\"CoCoOp\")\n",
        "  else:\n",
        "    load_model(base_model.prompt_learner, model_name=\"CoCoOp\", device=DEVICE)\n",
        "\n",
        "  # Load weights on novel model\n",
        "  novel_model.prompt_learner.ctx.data = base_model.prompt_learner.ctx.data.clone()\n",
        "  novel_model.prompt_learner.meta_net.load_state_dict(base_model.prompt_learner.meta_net.state_dict())\n",
        "\n",
        "  # Evaluate base and novel\n",
        "  base_accuracy = cocoop_test(base_model, test_base_loader, base_classes, DEVICE, label=\"🧠 CoCoOp evaluation on Base\")\n",
        "  novel_accuracy = cocoop_test(novel_model, test_novel_loader, novel_classes, DEVICE, label=\"🧠 CoCoOp evaluation on Novel\")\n",
        "\n",
        "  # Show results\n",
        "  print() # For separating from progress bars\n",
        "  print(\"=\"*20,\"Evaluating CoCoOp\",\"=\"*20)\n",
        "  print(f\"🔍 Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
        "  print(f\"🔍 Novel classes accuracy: {novel_accuracy*100:.2f}%\")\n",
        "  print(f\"🔍 Harmonic Mean: {harmonic_mean(base_accuracy, novel_accuracy)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2EK7zH_1m3v"
      },
      "source": [
        "Let's now see the results by executing the following evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "c8tfSDwL3Gzo"
      },
      "outputs": [],
      "source": [
        "if RUN_COCOOP:\n",
        "  cocoop_eval(do_train=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eYrYDGNwzXA"
      },
      "source": [
        "The new CoCoOp results can now be compared with zero-shot:\n",
        "<div align=\"center\">\n",
        "\n",
        "| Model      | Base (↑) | Novel (↑) | Harmonic Mean |\n",
        "|------------|----------|-----------|---------------|\n",
        "| Zero-Shot  | 71.33%   | 78.24%    | 74.62%        |\n",
        "| CoCoOp     | 95,19%   | 71.11%    | 81.41%        |\n",
        "\n",
        "<div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpbX_PmsHCGk"
      },
      "source": [
        "# Evaluation of CLIP-LoRA\n",
        "Before implementing the syntetic generation pipeline we will improve, CLIP needs to be adapted to avoid training all the parameters. The technique chosen is Low-Rank Adaption.\n",
        "\n",
        "This technique reduces the number of params to train and allows to mantain novel performance while increasing base one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Kh45gLgdibOY"
      },
      "outputs": [],
      "source": [
        "class LoRAGroupedLinear(nn.Linear):\n",
        "  \"\"\"Augmented linear layer using a group of low-rank updates.\"\"\"\n",
        "  def __init__(self, in_features, out_features, bias, lora_r, lora_alpha, lora_dropout, enable_lora=[False], merge_weights=True):\n",
        "    \"\"\"Initialize the module.\n",
        "    Args:\n",
        "      in_features (int): number of input features.\n",
        "      out_features (int): number of output features.\n",
        "      lora_r (int): rank of low rank decomposition.\n",
        "      lora_alpha (int): scaling factor for LoRA adjustment.\n",
        "      lora_dropout (float): dropout prob for dropout in LoRA branch.\n",
        "      enable_lora (list): list of which output groups should use LoRA.\n",
        "      merge_weights (bool): if true merge LoRA and model weights for inference.\n",
        "    \"\"\"\n",
        "    # Initialize parent linear layer\n",
        "    super().__init__(in_features, out_features, bias=bias)\n",
        "\n",
        "    # Save LoRA input params\n",
        "    self.lora_r = lora_r\n",
        "    self.lora_alpha = lora_alpha\n",
        "    self.lora_dropout = nn.Dropout(lora_dropout) if lora_dropout > 0 else nn.Identity()\n",
        "    self.merge_weights = merge_weights\n",
        "    self.enable_lora = enable_lora\n",
        "\n",
        "    # When initialization occurs weights are unmerged\n",
        "    self.merged=False\n",
        "\n",
        "    # Declare as many LoRA params as are the outputs\n",
        "    if lora_r > 0 and any(enable_lora):\n",
        "      self.n_lora_out = sum(enable_lora)\n",
        "      self.n_out = len(enable_lora)\n",
        "      self.lora_A = nn.Parameter(self.weight.new_zeros((lora_r * self.n_lora_out, in_features)))\n",
        "      self.lora_B = nn.Parameter(self.weight.new_zeros((out_features // self.n_out * self.n_lora_out, lora_r)))\n",
        "\n",
        "      self.scaling = self.lora_alpha / self.lora_r\n",
        "\n",
        "      # Freeze pretrained\n",
        "      self.weight.requires_grad = False\n",
        "\n",
        "      # Mask to track where LoRA is applied\n",
        "      self.lora_ind = self.weight.new_zeros((out_features), dtype=torch.bool).view(self.n_out, -1)\n",
        "      self.lora_ind[enable_lora, :] = True\n",
        "      self.lora_ind = self.lora_ind.view(-1)\n",
        "\n",
        "    # Initialize LoRA params\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    \"\"\"Method to reinit linear and LoRA params.\"\"\"\n",
        "    super().reset_parameters()\n",
        "    if hasattr(self, 'lora_A'):\n",
        "      # lora_A is initialized in a default way as nn.Linear\n",
        "      nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
        "      # lora_B is initialized to zero\n",
        "      nn.init.zeros_(self.lora_B)\n",
        "\n",
        "  def zero_pad(self, input_seq):\n",
        "    \"\"\"Zero pads the input sequence to match output feature dim using LoRA mask.\n",
        "    Args:\n",
        "      input_seq (Tensor): input sequence to pad.\n",
        "    Returns:\n",
        "      Tensor: padded input sequence.\n",
        "    \"\"\"\n",
        "    # First get zero tensor\n",
        "    padded = input_seq.new_zeros((len(self.lora_ind), *input_seq.shape[1:]))\n",
        "    # Substitute input sequence using LoRA mask\n",
        "    padded[self.lora_ind] = input_seq\n",
        "    return padded\n",
        "\n",
        "  def merge_AB(self):\n",
        "    \"\"\"Compute effective weight delta from lora_A and lora_B and apply zero padding.\"\"\"\n",
        "    weight_delta = F.conv1d(\n",
        "        self.lora_A.unsqueeze(0),\n",
        "        self.lora_B.unsqueeze(-1),\n",
        "        groups=self.n_lora_out\n",
        "    ).squeeze(0)\n",
        "    return self.zero_pad(weight_delta)\n",
        "\n",
        "  def train(self, mode=True):\n",
        "    \"\"\"Enable training mode. If merging is enabled, weights are unmerged when in training mode\n",
        "    and merged back when switching to eval mode.\"\"\"\n",
        "    super().train(mode)\n",
        "    if mode: # If training mode\n",
        "      if self.merge_weights and self.merged:\n",
        "        # If merging is active and weights are merged, unmerge\n",
        "        if self.lora_r > 0 and any(self.enable_lora):\n",
        "          self.weight.data -= self.merge_AB() * self.scaling\n",
        "        self.merged = False\n",
        "    else: # Going in eval mode\n",
        "      if self.merge_weights and not self.merged:\n",
        "        # If merging is active and weights are not merged, merge\n",
        "        if self.lora_r > 0 and any(self.enable_lora):\n",
        "          self.weight.data += self.merge_AB() * self.scaling\n",
        "        self.merged = True\n",
        "\n",
        "  def forward(self, input_seq):\n",
        "    \"\"\"Forward pass.\n",
        "    Args:\n",
        "      input_seq (Tensor): input sequence.\n",
        "    Returns:\n",
        "      Tensor: output sequence.\n",
        "    \"\"\"\n",
        "    output = F.linear(input_seq, self.weight, bias=self.bias)\n",
        "    if self.merged: # If weights are merged -> eval mode\n",
        "      return output\n",
        "    else: # weights not merged -> train mode\n",
        "      if self.lora_r > 0:\n",
        "        lora_out = self.lora_dropout(input_seq) @ self.merge_AB().T\n",
        "        output += lora_out * self.scaling\n",
        "      return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwoTRjPupzQn"
      },
      "source": [
        "Multihead attention layer must be adapted for LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "AyV_o307KTB0"
      },
      "outputs": [],
      "source": [
        "class LoRAMultiHeadAttention(nn.Module):\n",
        "  \"\"\"Multi-Head Attention module augmented with LoRA.\"\"\"\n",
        "  def __init__(self, lora_r, lora_alpha, lora_dropout, embed_dim, num_heads, dropout, bias=True, q_lora=True, k_lora=False, v_lora=True):\n",
        "    \"\"\"\"Initialize the module.\n",
        "    Args:\n",
        "      lora_r (int): rank of LoRA update matrices.\n",
        "      lora_alpha (int): scaling factor for LoRA.\n",
        "      lora_dropout (float): dropout probability for LoRA.\n",
        "      embed_dim (int): dimension of input embeddings.\n",
        "      num_heads (int): number of attention heads.\n",
        "      dropout (float): dropout probability after attention.\n",
        "      bias (bool): if True add a learnable bias to projection layers.\n",
        "      q_lora (bool): enable LoRA for query projection.\n",
        "      k_lora (bool): enable LoRA for key projection.\n",
        "      v_lora (bool): enable LoRA for value projection.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # Save embed dim and set dim of key and value vectors equal to it\n",
        "    self.embed_dim = embed_dim\n",
        "    self.kdim = embed_dim\n",
        "    self.vdim = embed_dim\n",
        "\n",
        "    # Save multihead attention params\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout = dropout\n",
        "    # Dimension of an head in embed_dim / num_heads\n",
        "    self.head_dim = embed_dim // num_heads\n",
        "\n",
        "    # Merged linear layer with LoRA\n",
        "    qkv_params = {\n",
        "        \"in_features\": embed_dim,\n",
        "        \"out_features\": 3 * embed_dim, # since we get three outputs for query, key and values\n",
        "        \"bias\": bias, # enable bias for linear layer\n",
        "        \"lora_r\": lora_r,\n",
        "        \"lora_alpha\": lora_alpha,\n",
        "        \"lora_dropout\": lora_dropout,\n",
        "        \"enable_lora\": [q_lora, k_lora, v_lora]\n",
        "    }\n",
        "    self.qkv = LoRAGroupedLinear(**qkv_params)\n",
        "\n",
        "    self.scaled_dot_product_attention = F.scaled_dot_product_attention\n",
        "    self.proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "\n",
        "  def set_parameters(self, mod):\n",
        "    \"\"\"Initialize from existing Pytorch modules to load pretrained weights.\n",
        "    Args:\n",
        "      mod (nn.Module): Pytorch module with pretrained weights.\n",
        "    \"\"\"\n",
        "    # Copy weights\n",
        "    self.qkv.weight.data = mod.in_proj_weight.data\n",
        "    self.qkv.bias.data = mod.in_proj_bias.data\n",
        "    self.proj.weight.data = mod.out_proj.weight.data\n",
        "    self.proj.bias.data = mod.out_proj.bias.data\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, query, key, value, key_padding_mask=None, attn_mask=None, is_causal=False, need_weights=True):\n",
        "    \"\"\"Forward pass to apply Multi-Head Attention.\n",
        "    Args:\n",
        "      query (Tensor): query tensor.\n",
        "      key (Tensor): key tensor.\n",
        "      value (Tensor): value tensor.\n",
        "      key_padding_mask (Tensor): mask to ignore padding.\n",
        "      attn_mask (Tensor): optinal attention mask.\n",
        "      is_causal (bool): If True, applies a causal mask.\n",
        "      need_weights (bool): parameter kept for compatibility.\n",
        "    \"\"\"\n",
        "    # Create key padding mask of proper shape and size\n",
        "    key_padding_mask = F._canonical_mask(\n",
        "      mask=key_padding_mask,\n",
        "      mask_name=\"key_padding_mask\",\n",
        "      other_type=F._none_or_dtype(attn_mask),\n",
        "      other_name=\"attn_mask\",\n",
        "      target_type=query.dtype,\n",
        "    )\n",
        "\n",
        "    # Get dims\n",
        "    tgt_len, bsz, embed_dim = query.shape\n",
        "    src_len, _, _ = key.shape\n",
        "    E = query.size(-1)\n",
        "\n",
        "    # Project Q, K, V using grouped LoRA linear layer\n",
        "    qkv = self.qkv(query)\n",
        "    # Split qkv from merged Tensor\n",
        "    qkv = qkv.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()\n",
        "    q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "    # Canonicalize attention mask\n",
        "    attn_mask = F._canonical_mask(\n",
        "      mask=attn_mask,\n",
        "      mask_name=\"attn_mask\",\n",
        "      other_type=F._none_or_dtype(key_padding_mask),\n",
        "      other_name=\"key_padding_mask\",\n",
        "      target_type=q.dtype,\n",
        "      check_other=False,\n",
        "    )\n",
        "\n",
        "    # If attn_mask param is provided, validate and reshape it\n",
        "    if attn_mask is not None:\n",
        "      # Mask dim must be 3\n",
        "      if attn_mask.dim() == 2:\n",
        "        attn_mask = attn_mask.unsqueeze(0)\n",
        "\n",
        "    if attn_mask is not None:\n",
        "      if attn_mask.size(0) == 1 and attn_mask.dim() == 3:\n",
        "        attn_mask = attn_mask.unsqueeze(0)\n",
        "      else:\n",
        "        attn_mask = attn_mask.view(bsz, self.num_heads, -1, src_len)\n",
        "\n",
        "    # Use dropout only if in training mode\n",
        "    dropout_p = self.dropout if self.training else 0.0\n",
        "\n",
        "    # Prepare q, k, v for attention by splitting heads\n",
        "    q = q.view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "    k = k.view(src_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "    v = v.view(src_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "    src_len = k.size(1)\n",
        "\n",
        "    # Reshape for attention\n",
        "    q = q.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
        "    k = k.view(bsz, self.num_heads, src_len, self.head_dim)\n",
        "    v = v.view(bsz, self.num_heads, src_len, self.head_dim)\n",
        "\n",
        "    # Apply scaled dot product attention\n",
        "    attn_output = self.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
        "\n",
        "    # Recombine attention heads\n",
        "    attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)\n",
        "\n",
        "    # Final output projection\n",
        "    attn_output = self.proj(attn_output)\n",
        "    attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n",
        "\n",
        "    return attn_output, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHTuWcNkcPfT"
      },
      "source": [
        "In order for the modules to be properly loaded a function to replace multihead modules in clip must be defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Uq5tx2GrcXgf"
      },
      "outputs": [],
      "source": [
        "def lora_replace_multihead_attention(transformer, lora_r, lora_alpha, lora_dropout):\n",
        "  \"\"\"Replace multihead attention in clip model with lora enhanced multihead attention.\n",
        "  All blocks are replaced in this implementation.\n",
        "  Args:\n",
        "    transformer (nn.Module): transformer model where to replace layers.\n",
        "    lora_r (int): rank of LoRA update matrices.\n",
        "    lora_alpha (int): scaling factor for LoRA.\n",
        "    lora_dropout (float): dropout probability for LoRA.\n",
        "  \"\"\"\n",
        "  for resblock in transformer.resblocks:\n",
        "    # Params taken from resblock\n",
        "    # References applies LoRA only to q and v\n",
        "    lora_multihead_params = {\n",
        "        \"lora_r\": lora_r,\n",
        "        \"lora_alpha\": lora_alpha,\n",
        "        \"lora_dropout\": lora_dropout,\n",
        "        \"embed_dim\": resblock.attn.embed_dim,\n",
        "        \"num_heads\": resblock.attn.num_heads,\n",
        "        \"dropout\": resblock.attn.dropout,\n",
        "        \"bias\": True,\n",
        "        \"q_lora\": True,\n",
        "        \"k_lora\": False,\n",
        "        \"v_lora\": True\n",
        "    }\n",
        "    # Initialize, load weights and substitute module\n",
        "    lora_attn = LoRAMultiHeadAttention(**lora_multihead_params)\n",
        "    lora_attn.set_parameters(resblock.attn)\n",
        "    resblock.attn = lora_attn\n",
        "\n",
        "  return transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "b7rGa5BAlgOo"
      },
      "outputs": [],
      "source": [
        "def get_clip_lora():\n",
        "  \"\"\"Load CLIP substituting LoRA modules into text and visual encoders.\"\"\"\n",
        "  # LoRA hyperparams taken from reference work\n",
        "  LORA_VISUAL_R = 64\n",
        "  LORA_VISUAL_ALPHA = 32\n",
        "  LORA_VISUAL_DROPOUT = 0.1\n",
        "  LORA_TEXT_R = 16\n",
        "  LORA_TEXT_ALPHA = 32\n",
        "  LORA_TEXT_DROPOUT = 0.1\n",
        "\n",
        "  clip_model, _ = clip.load(CLIP_BACKBONE, device=\"cpu\")\n",
        "\n",
        "  # Apply LoRA to both text and visual encoders\n",
        "  clip_model.visual.transformer = lora_replace_multihead_attention(clip_model.visual.transformer,\n",
        "    lora_r=LORA_VISUAL_R, lora_alpha=LORA_VISUAL_ALPHA, lora_dropout=LORA_VISUAL_DROPOUT\n",
        "  )\n",
        "  clip_model.transformer = lora_replace_multihead_attention(clip_model.transformer,\n",
        "    lora_r=LORA_TEXT_R, lora_alpha=LORA_TEXT_ALPHA, lora_dropout=LORA_TEXT_DROPOUT\n",
        "  )\n",
        "  clip_model = clip_model.to(DEVICE)\n",
        "\n",
        "  # Freeze CLIP params\n",
        "  for name, param in clip_model.named_parameters():\n",
        "      param.requires_grad = 'lora' in name\n",
        "\n",
        "  # Sanity check, only LoRA params to train\n",
        "  trainable = {name for name, param in clip_model.named_parameters() if param.requires_grad}\n",
        "  print(f\"Trainable params: {trainable}\")\n",
        "\n",
        "  return clip_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvjglQRF-EYm"
      },
      "source": [
        "Since we want to save only LoRA weights and not all CLIP weights we require some utily function to save and load them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "_NjhBF7g-EMP"
      },
      "outputs": [],
      "source": [
        "def save_lora(model):\n",
        "  \"\"\"Given a CLIP-LoRA model, save only LoRA weights.\n",
        "  Args:\n",
        "    model (nn.Module): model to save.\n",
        "  \"\"\"\n",
        "  save_dir = os.path.join(\"bin\")\n",
        "  os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "  lora_state_dict = {\n",
        "    k: v for k, v in model.state_dict().items()\n",
        "    if model.get_parameter(k).requires_grad\n",
        "  }\n",
        "  file_path = os.path.join(save_dir, \"LoRA.pt\")\n",
        "  torch.save(lora_state_dict, file_path)\n",
        "  print(f\"LoRA weights saved to {file_path}.\")\n",
        "\n",
        "def load_lora(model, device=\"cuda:0\"):\n",
        "  \"\"\"Load into model LoRA only weights.\n",
        "  Args:\n",
        "    model (nn.Module): model to load weights.\n",
        "    device (str): device to use.\n",
        "  \"\"\"\n",
        "  file_path = os.path.join(\"bin\", \"LoRA.pt\")\n",
        "\n",
        "  if not os.path.isfile(file_path):\n",
        "    raise FileNotFoundError(f\"No LoRA weights file found at {file_path}\")\n",
        "\n",
        "  lora_state_dict = torch.load(file_path, map_location=torch.device(device))\n",
        "\n",
        "  # Only load the matching keys\n",
        "  model.load_state_dict(lora_state_dict, strict=False)\n",
        "  print(f\"LoRA weights loaded from {file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V4L7CL2ggPL"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "hm1ikf74i1V3"
      },
      "outputs": [],
      "source": [
        "def lora_train(lora_model, train_loader, val_loader, optimizer, loss_fun, scheduler, base_classes, num_epochs=5, patience=3, device=\"cuda:0\"):\n",
        "  \"\"\"\"Train CLIP-LoRA model.\n",
        "  Args:\n",
        "    lora_model (nn.Module): CLIP-LoRA model to train.\n",
        "    train_loader (DataLoader): training data loader.\n",
        "    val_loader (DataLoader): validation data loader.\n",
        "    optimizer (torch.optim): optimizer to use.\n",
        "    loss_fun (nn.Module): loss function to use.\n",
        "    scheduler (torch.optim.lr_scheduler): learning rate scheduler.\n",
        "    base_classes (list): list of base classes.\n",
        "    num_epochs (int): number of epochs to train.\n",
        "    patience (int): patience for early stopping.\n",
        "    device (str): device to use.\n",
        "  \"\"\"\n",
        "  def train_step(lora_model, loader, optimizer, loss_fun, base_classes, device=\"cuda\"):\n",
        "    \"\"\"Train CLIP-LoRA model for one epoch.\n",
        "    Args:\n",
        "      lora_model (nn.Module): CLIP-LoRA model to train.\n",
        "      loader (DataLoader): train data loader.\n",
        "      optimizer (torch.optim): optimizer to use.\n",
        "      loss_fun (nn.Module): loss function to use.\n",
        "      base_classes (list): list of base classes.\n",
        "      device (str): device to use.\n",
        "    \"\"\"\n",
        "    # Variables to compute performance scores\n",
        "    samples = 0\n",
        "    cumulative_loss = 0\n",
        "    cumulative_accuracy = 0\n",
        "\n",
        "    # Put model in training mode\n",
        "    lora_model.train()\n",
        "\n",
        "    # Iterate over training set\n",
        "    for images, labels in loader:\n",
        "      # Put images and labels on correct device\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "      # Encode images and normalize image features\n",
        "      image_features = lora_model.encode_image(images)\n",
        "      image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "      # Encode text and normalize text features\n",
        "      text_prompts = [PROMPT_FORMAT.format(CLASS_NAMES[cls]) for cls in base_classes]\n",
        "      text_inputs = clip.tokenize(text_prompts).to(device)\n",
        "      text_features = lora_model.encode_text(text_inputs)\n",
        "      text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "      # Compute similarity logits\n",
        "      logit_scale = 100 # This value is used with LoRA\n",
        "      logits = logit_scale * image_features @ text_features.T\n",
        "      # Targets -> image i should match with text prompt j (contrastive learning idea)\n",
        "      #targets = torch.arange(len(images)).to(device)\n",
        "\n",
        "      loss = loss_fun(logits, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Compute performance scores\n",
        "      batch_size = images.shape[0]\n",
        "      samples += batch_size\n",
        "      cumulative_loss += loss.item() * batch_size\n",
        "      _, predicted = logits.max(dim=1)\n",
        "      cumulative_accuracy += predicted.eq(labels).sum().item()\n",
        "\n",
        "    loss = cumulative_loss / samples\n",
        "    accuracy = cumulative_accuracy / samples\n",
        "    return loss, accuracy\n",
        "\n",
        "  # Initalize training vars before train loop\n",
        "  best_val_acc = float('-inf')\n",
        "  best_model_state = None\n",
        "  curr_patience = patience\n",
        "  pb = tqdm(range(num_epochs))\n",
        "\n",
        "  for epoch in pb:\n",
        "    # Training step\n",
        "    train_loss, train_acc = train_step(lora_model, train_loader, optimizer, loss_fun, base_classes, device)\n",
        "\n",
        "    # Evaluation step\n",
        "    val_acc = clip_test(lora_model, val_loader, base_classes, device)\n",
        "\n",
        "    # Show val accuracy\n",
        "    pb.set_description(f\"Val Acc: {val_acc*100:.2f}%\")\n",
        "\n",
        "    # If we get better metric save the model\n",
        "    if val_acc > best_val_acc:\n",
        "      best_val_acc = val_acc\n",
        "      best_model_state = lora_model.state_dict()\n",
        "      curr_patience = patience\n",
        "    else:\n",
        "      if curr_patience < 1:\n",
        "        break\n",
        "      else:\n",
        "        curr_patience = curr_patience - 1\n",
        "\n",
        "    # If scheduler is given, scheduler step\n",
        "    if scheduler is not None:\n",
        "      scheduler.step()\n",
        "\n",
        "  # Load best model before returning\n",
        "  if best_model_state is not None:\n",
        "    lora_model.load_state_dict(best_model_state)\n",
        "  return lora_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "hBeDs-3FY5Yv"
      },
      "outputs": [],
      "source": [
        "def lora_eval(do_train=True):\n",
        "  \"\"\"Evaluate CLIP-LoRA.\n",
        "  Args:\n",
        "    do_train (bool): train model or load weights.\n",
        "  \"\"\"\n",
        "  # Get CLIP-LoRA\n",
        "  clip_model = get_clip_lora()\n",
        "\n",
        "  # Get datasets\n",
        "  train_base, val_base, test_base, test_novel, base_classes, novel_classes = get_dataset(do_preprocess=True)\n",
        "\n",
        "  # Hyperparam\n",
        "  TRAIN_BATCH = 32 # From CLIP-LoRA paper\n",
        "  VAL_BATCH = 64\n",
        "  TEST_BATCH = 128\n",
        "  LR = 2e-4 # From CLIP-LoRA paper\n",
        "  NUM_EPOCHS = 15\n",
        "  PATIENCE = 3\n",
        "\n",
        "  # Get loaders\n",
        "  train_loader = torch.utils.data.DataLoader(train_base, batch_size=TRAIN_BATCH, shuffle=True, num_workers=2)\n",
        "  val_loader = torch.utils.data.DataLoader(val_base, batch_size=VAL_BATCH, shuffle=False, num_workers=2)\n",
        "  test_base_loader = torch.utils.data.DataLoader(test_base, batch_size=TEST_BATCH, shuffle=False, num_workers=2)\n",
        "  test_novel_loader = torch.utils.data.DataLoader(test_novel, batch_size=TEST_BATCH, shuffle=False, num_workers=2)\n",
        "\n",
        "  # Get cost function\n",
        "  loss_fun = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  # Train\n",
        "  if do_train:\n",
        "    optimizer = optim.AdamW([p for p in clip_model.parameters() if p.requires_grad], lr=LR)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
        "\n",
        "    # Train\n",
        "    train_params = {\n",
        "        \"lora_model\": clip_model,\n",
        "        \"train_loader\": train_loader,\n",
        "        \"val_loader\": val_loader,\n",
        "        \"optimizer\": optimizer,\n",
        "        \"loss_fun\": loss_fun,\n",
        "        \"scheduler\": scheduler,\n",
        "        \"base_classes\": base_classes,\n",
        "        \"num_epochs\": NUM_EPOCHS,\n",
        "        \"patience\": PATIENCE,\n",
        "        \"device\": DEVICE\n",
        "    }\n",
        "    clip_model = lora_train(**train_params)\n",
        "    # Save weights\n",
        "    save_lora(clip_model)\n",
        "  else:\n",
        "    load_lora(clip_model, device=DEVICE)\n",
        "\n",
        "  # Evaluate base and novel\n",
        "  base_accuracy = clip_test(clip_model, test_base_loader, base_classes, DEVICE, label=\"🧠 LoRA evaluation on Base\")\n",
        "  novel_accuracy = clip_test(clip_model, test_novel_loader, novel_classes, DEVICE, label=\"🧠 LoRA evaluation on Novel\")\n",
        "\n",
        "  # Show results\n",
        "  print() # For separating from progress bars\n",
        "  print(\"=\"*20,\"Evaluating LoRA\",\"=\"*20)\n",
        "  print(f\"🔍 Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
        "  print(f\"🔍 Novel classes accuracy: {novel_accuracy*100:.2f}%\")\n",
        "  print(f\"🔍 Harmonic Mean: {harmonic_mean(base_accuracy, novel_accuracy)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCtrOylIteNP"
      },
      "source": [
        "Now LoRA can be properly evaluated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "AccxUuVJ8l1Q"
      },
      "outputs": [],
      "source": [
        "if RUN_LORA:\n",
        "  lora_eval(do_train=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVsfJeaztjwn"
      },
      "source": [
        "The new CoCoOp results can now be compared with zero-shot:\n",
        "<div align=\"center\">\n",
        "\n",
        "| Model      | Base (↑) | Novel (↑) | Harmonic Mean |\n",
        "|------------|----------|-----------|---------------|\n",
        "| Zero-Shot  | 71.33%   | 78.24%    | 74.62%        |\n",
        "| CoCoOp     | 95,19%   | 71.11%    | 81.41%        |\n",
        "| CLIP-LoRA     | 96.81 %   | 74.13 %    | 83.96 %        |\n",
        "\n",
        "<div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkpEQgsv2dL0"
      },
      "source": [
        "# Evaluation of DISEF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_BJe9EfVpdk"
      },
      "source": [
        "## Synthetic sample generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "rOVpS8WIVtU-"
      },
      "outputs": [],
      "source": [
        "# ADD SAMPLE GENERATION CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y0mqzn4Vtsy"
      },
      "source": [
        "## Training and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbC1XM5_V11C"
      },
      "source": [
        "The synthetic dataset must be loaded from current `imgs` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Ue6UkKL2VuRP"
      },
      "outputs": [],
      "source": [
        "def load_syn_dataset(do_preprocess=False):\n",
        "  \"\"\"Load generated samples for training.\n",
        "  Args:\n",
        "    do_preprocess (bool): if True preprocess with CLIP preprocess.\n",
        "  \"\"\"\n",
        "  # Collect samples in pairs (img, label)\n",
        "  syn_dataset = []\n",
        "\n",
        "  # Get clip preprocess\n",
        "  if do_preprocess:\n",
        "    _, preprocess = clip.load(CLIP_BACKBONE, device=\"cpu\")\n",
        "\n",
        "  for label, class_name in enumerate(CLASS_NAMES):\n",
        "    current_dir = os.path.join(\"imgs\", class_name)\n",
        "    # If no folder for a class_name\n",
        "    if not os.path.isdir(current_dir):\n",
        "      continue\n",
        "    # Iterate on img in class folder\n",
        "    for img_name in os.path.listdir(current_dir):\n",
        "      img_path = os.path.join(class_dir, img_name)\n",
        "\n",
        "      img = Image.open(img_path).convert(\"RGB\")\n",
        "      if do_preprocess:\n",
        "        img = preprocess(img)\n",
        "      syn_dataset.append((img, label))\n",
        "  return syn_dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZAfEeO9aFNW"
      },
      "source": [
        "A custom training function is used by DISEF. Also a custom loss is computed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "7H9iQFbsZ8LD"
      },
      "outputs": [],
      "source": [
        "def disef_train(lora_model, train_loader, syn_loader, val_loader, optimizer, loss_fun, scheduler, base_classes, num_epochs=5, patience=3, device=\"cuda:0\"):\n",
        "  \"\"\"\"Training a CLIP-LoRA model using synthetic samples.\n",
        "  Args:\n",
        "    lora_model (nn.Module): CLIP-LoRA model to train.\n",
        "    train_loader (DataLoader): training dataloader.\n",
        "    syn_loader (DataLoader): synthetic training dataloader.\n",
        "    val_loader (DataLoader): validation dataloader.\n",
        "    optimizer (torch.optim): optimizer to use.\n",
        "    loss_fun (nn.Module): loss function to use.\n",
        "    scheduler (torch.optim.lr_scheduler): learning rate scheduler.\n",
        "    base_classes (list): list of base classes.\n",
        "    num_epochs (int): number of epochs to train.\n",
        "    patience (int): patience for early stopping.\n",
        "    device (str): device to use.\n",
        "  \"\"\"\n",
        "  def train_step(lora_model, real_loader, syn_loader, optimizer, loss_fun, base_classes, lambda_weight=0.8, device=\"cuda\"):\n",
        "    \"\"\"Train CLIP-LoRA model for one epoch.\n",
        "    Args:\n",
        "      lora_model (nn.Module): CLIP-LoRA model to train.\n",
        "      real_loader (DataLoader): real samples training dataloader.\n",
        "      syn_loader (DataLoader): synthetic samples training dataloader.\n",
        "      optimizer (torch.optim): optimizer to use.\n",
        "      loss_fun (nn.Module): loss function to use.\n",
        "      base_classes (list): list of base classes.\n",
        "      lambda_weight (float): weight to compute the loss.\n",
        "      device (str): device to use.\n",
        "    \"\"\"\n",
        "    # Variables to compute performance scores\n",
        "    samples = 0\n",
        "    cumulative_loss = 0\n",
        "    cumulative_accuracy = 0\n",
        "\n",
        "    # Set training mode\n",
        "    lora_model.train()\n",
        "\n",
        "    assert len(real_loader) == len(syn_loader), \"real and synthetic loaders must have same batch number\"\n",
        "\n",
        "    # Iterate over training sets\n",
        "    for (real_images, real_labels), (syn_images, syn_labels) in zip(real_loader, syn_loader):\n",
        "      # Move to device\n",
        "      images_real, labels_real = images_real.to(device), labels_real.to(device)\n",
        "      images_syn, labels_syn = images_syn.to(device), labels_syn.to(device)\n",
        "\n",
        "      # Real visual features\n",
        "      fv_real = lora_model.encode_image(images_real)\n",
        "      fv_real = fv_real / fv_real.norm(dim=-1, keepdim=True)\n",
        "\n",
        "      # Synthetic visual features\n",
        "      fv_syn = lora_model.encode_image(images_syn)\n",
        "      fv_syn = fv_syn / fv_syn.norm(dim=-1, keepdim=True)\n",
        "\n",
        "      # Text features for all classes\n",
        "      text_prompts = [PROMPT_FORMAT.format(CLASS_NAMES[cls]) for cls in base_classes]\n",
        "      text_tokens = clip.tokenize(text_prompts).to(device)\n",
        "      ft = lora_model.encode_text(text_tokens)\n",
        "      ft = ft / ft.norm(dim=-1, keepdim=True)\n",
        "\n",
        "      logit_scale = 100 # Used with CLIP-LoRA\n",
        "      logits_real = logit_scale * fv_real @ ft.T\n",
        "      logits_syn = logit_scale * fv_syn @ ft.T\n",
        "\n",
        "      loss_real = loss_fun(logits_real, labels_real)\n",
        "      loss_syn = loss_fun(logits_syn, labels_syn)\n",
        "      loss = lambda_weight * loss_real + (1 - lambda_weight) * loss_syn\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      batch_size = images_real.size(0) + images_syn.size(0)\n",
        "      cumulative_loss += loss.item() * batch_size\n",
        "      samples += batch_size\n",
        "\n",
        "      pred_real = logits_real.argmax(dim=1)\n",
        "      pred_syn = logits_syn.argmax(dim=1)\n",
        "      cumulative_accuracy += (pred_real == labels_real).sum().item() + (pred_syn == labels_syn).sum().item()\n",
        "\n",
        "    loss = cumulative_loss / samples\n",
        "    accuracy = cumulative_accuracy / samples\n",
        "    return loss, accuracy\n",
        "\n",
        "  # Initialization before training loop\n",
        "  best_val_acc = float('-inf')\n",
        "  best_model_state = None\n",
        "  curr_patience = patience\n",
        "  pb = tqdm(range(num_epochs))\n",
        "\n",
        "  # Logging for plots\n",
        "  log = {\n",
        "      \"epoch\": [],\n",
        "      \"train_loss\": [],\n",
        "      \"train_acc\": [],\n",
        "      \"val_acc\": []\n",
        "  }\n",
        "\n",
        "  for epoch in pb:\n",
        "    # Training step\n",
        "    lambda_weight = 0.8 # From DISEF paper\n",
        "    train_loss, train_acc = train_step(lora_model, train_loader, syn_loader, optimizer, loss_fun, base_classes, lambda_weight, device)\n",
        "\n",
        "    # Evaluation step\n",
        "    val_acc = clip_test(lora_model, val_loader, base_classes, device)\n",
        "\n",
        "    # Logging\n",
        "    log[\"epoch\"].append(epoch)\n",
        "    log[\"train_loss\"].append(train_loss)\n",
        "    log[\"train_acc\"].append(train_acc)\n",
        "    log[\"val_acc\"].append(val_acc)\n",
        "\n",
        "    # Show val accuracy\n",
        "    pb.set_description(f\"Val Acc: {val_acc*100:.2f}%\")\n",
        "\n",
        "    # If we get better metric save the model\n",
        "    if val_acc > best_val_acc:\n",
        "      best_val_acc = val_acc\n",
        "      best_model_state = lora_model.state_dict()\n",
        "      curr_patience = patience\n",
        "    else:\n",
        "      if curr_patience < 1:\n",
        "        break\n",
        "      else:\n",
        "        curr_patience = curr_patience - 1\n",
        "\n",
        "    # If scheduler is given, scheduler step\n",
        "    if scheduler is not None:\n",
        "      scheduler.step()\n",
        "\n",
        "  # Load best model before returning\n",
        "  if best_model_state is not None:\n",
        "    lora_model.load_state_dict(best_model_state)\n",
        "  return lora_model, log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o7ZcA0epHqe"
      },
      "source": [
        "A plotting function is provided to understand the performance changes through training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "V-gjcpl4o8o9"
      },
      "outputs": [],
      "source": [
        "def plot_log(log):\n",
        "  \"\"\"\"Plot training log.\n",
        "  Args:\n",
        "    log (dict): training log.\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(15,5))\n",
        "\n",
        "  # Subplot for Cross Entropy\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(log[\"epoch\"], log[\"train_loss\"], label=\"Training Loss\", color=\"#6c757d\")\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Cross Entropy Loss')\n",
        "  plt.title('Training Loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "  # Subplot for accuracy\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(log[\"epoch\"], log[\"train_acc\"], label=\"Train Acc\", color=\"#6c757d\")\n",
        "  plt.plot(log[\"epoch\"], log[\"val_acc\"], label=\"Valid Acc\", color=\"#e9c46a\")\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.title('Training and Validation Accuracy')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Save in current directory\n",
        "  plt.savefig()\n",
        "  print(\"Training plot saved in current path.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "UhsJKFNAnQzr"
      },
      "outputs": [],
      "source": [
        "def disef_eval(do_gen=True, do_train=True):\n",
        "  \"\"\"\"Evaluate DISEF technique.\n",
        "  Args:\n",
        "    do_gen (bool): generate synthetic samples or not.\n",
        "    do_train (bool): train model or load weights.\n",
        "  \"\"\"\n",
        "\n",
        "  if do_gen:\n",
        "    # Generate synthetic dataset\n",
        "    # generate_samples()\n",
        "    pass\n",
        "\n",
        "  syn_dataset = load_syn_dataset(do_preprocess=True)\n",
        "\n",
        "  # Get CLIP-LoRA\n",
        "  clip_model = get_clip_lora()\n",
        "\n",
        "  # Get datasets\n",
        "  train_base, val_base, test_base, test_novel, base_classes, novel_classes = get_dataset(do_preprocess=True)\n",
        "\n",
        "  # Hyperparam\n",
        "  TRAIN_BATCH = 32 # From CLIP-LoRA paper\n",
        "  VAL_BATCH = 64\n",
        "  TEST_BATCH = 128\n",
        "  LR = 2e-4 # From CLIP-LoRA paper\n",
        "  NUM_EPOCHS = 15\n",
        "  PATIENCE = 3\n",
        "\n",
        "  # Get loaders\n",
        "  train_loader = torch.utils.data.DataLoader(train_base, batch_size=TRAIN_BATCH, shuffle=True, num_workers=2)\n",
        "  syn_loader = torch.utils.data.DataLoader(syn_dataset, batch_size=TRAIN_BATCH, shuffle=True, num_workers=2)\n",
        "  val_loader = torch.utils.data.DataLoader(val_base, batch_size=VAL_BATCH, shuffle=False, num_workers=2)\n",
        "  test_base_loader = torch.utils.data.DataLoader(test_base, batch_size=TEST_BATCH, shuffle=False, num_workers=2)\n",
        "  test_novel_loader = torch.utils.data.DataLoader(test_novel, batch_size=TEST_BATCH, shuffle=False, num_workers=2)\n",
        "\n",
        "  # Get cost function\n",
        "  loss_fun = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  # Train\n",
        "  if do_train:\n",
        "    optimizer = optim.AdamW([p for p in clip_model.parameters() if p.requires_grad], lr=LR)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
        "\n",
        "    # Train\n",
        "    train_params = {\n",
        "        \"lora_model\": clip_model,\n",
        "        \"train_loader\": train_loader,\n",
        "        \"syn_loader\": syn_loader,\n",
        "        \"val_loader\": val_loader,\n",
        "        \"optimizer\": optimizer,\n",
        "        \"loss_fun\": loss_fun,\n",
        "        \"scheduler\": scheduler,\n",
        "        \"base_classes\": base_classes,\n",
        "        \"num_epochs\": NUM_EPOCHS,\n",
        "        \"patience\": PATIENCE,\n",
        "        \"device\": DEVICE\n",
        "    }\n",
        "    clip_model, log = lora_train(**train_params)\n",
        "    # Get training plot\n",
        "    plot_log(log)\n",
        "    # Save weights\n",
        "    save_lora(clip_model)\n",
        "  else:\n",
        "    load_lora(clip_model, device=DEVICE)\n",
        "\n",
        "  # Evaluate base and novel\n",
        "  base_accuracy = clip_test(clip_model, test_base_loader, base_classes, DEVICE, label=\"🧠 DISEF evaluation on Base\")\n",
        "  novel_accuracy = clip_test(clip_model, test_novel_loader, novel_classes, DEVICE, label=\"🧠 DISEF evaluation on Novel\")\n",
        "\n",
        "  # Show results\n",
        "  print() # For separating from progress bars\n",
        "  print(\"=\"*20,\"Evaluating DISEF\",\"=\"*20)\n",
        "  print(f\"🔍 Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
        "  print(f\"🔍 Novel classes accuracy: {novel_accuracy*100:.2f}%\")\n",
        "  print(f\"🔍 Harmonic Mean: {harmonic_mean(base_accuracy, novel_accuracy)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "IF3aJf9SnXXW"
      },
      "outputs": [],
      "source": [
        "if RUN_DISEF:\n",
        "  disef_eval(do_gen=True, do_train=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGG5BMMSnRJT"
      },
      "source": [
        "The new CoCoOp results can now be compared with zero-shot:\n",
        "<div align=\"center\">\n",
        "\n",
        "| Model      | Base (↑) | Novel (↑) | Harmonic Mean |\n",
        "|------------|----------|-----------|---------------|\n",
        "| Zero-Shot  | 71.33%   | 78.24%    | 74.62%        |\n",
        "| CoCoOp     | 95,19%   | 71.11%    | 81.41%        |\n",
        "| CLIP-LoRA     | 96.81 %   | 74.13 %    | 83.96 %        |\n",
        "| DISEF     |  %   |  %    |  %        |\n",
        "\n",
        "<div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uOnJZetxK7X"
      },
      "source": [
        "# Evaluation of improved DISEF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yGkTbDarAAL"
      },
      "source": [
        "The proposal is to compute f1 scores for every class using zero-shot clip to understand what are the most problematic classes for CLIP to predict and then adjust the generation accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgUwS7dt0kj7"
      },
      "source": [
        "## Synthetic sample generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "l8xvs9aprPAz"
      },
      "outputs": [],
      "source": [
        "def get_f1_scores(model, loader, categories, device=\"cuda:0\", label=\"\"):\n",
        "  \"\"\"\"Return class f1 scores given a model and a DataLoader.\n",
        "  Args:\n",
        "    model (nn.Module): CLIP model to compute f1 scores.\n",
        "    loader (DataLoader): DataLoader to test on.\n",
        "    categories (list): list of categories.\n",
        "    device (str): device to use.\n",
        "    label (str): label for progress bar.\n",
        "  Returns:\n",
        "    dict: Dictionary mapping label to F1 score.\n",
        "  \"\"\"\n",
        "  # Model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # Mappings for prediction\n",
        "  contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
        "  idx2cat = {v: k for k, v in contig_cat2idx.items()}  # Reverse map for output\n",
        "\n",
        "  # Save predictions and target labels\n",
        "  all_preds = []\n",
        "  all_tgts = []\n",
        "\n",
        "  # Tokenized inputs for CLIP\n",
        "  text_inputs = clip.tokenize([PROMPT_FORMAT.format(CLASS_NAMES[c]) for c in categories]).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # Text features\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    for images, labels in tqdm(loader, desc=label):\n",
        "      # Map labels to contiguous set\n",
        "      labels = torch.tensor([contig_cat2idx[l.item()] for l in labels]).long()\n",
        "\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      # Image features\n",
        "      image_features = model.encode_image(images)\n",
        "      image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "      logits = image_features @ text_features.T\n",
        "      predicted_class = logits.argmax(dim=-1)\n",
        "\n",
        "      all_preds.extend(predicted_class.to(\"cpu\").numpy())\n",
        "      all_tgts.extend(labels.to(\"cpu\").numpy())\n",
        "\n",
        "  # Use sklearn to compute metric\n",
        "  precision, recall, f1, _ = precision_recall_fscore_support(all_tgts, all_preds, labels=list(range(len(categories))), zero_division=0)\n",
        "\n",
        "  # Map class index to F1 score\n",
        "  f1_per_class = {CLASS_NAMES[idx2cat[i]]: f1[i] for i in range(len(categories))}\n",
        "\n",
        "  return f1_per_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvewoJmS2f9v",
        "outputId": "67382602-f1be-4d48-c09f-fb7ac4e3fcd6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Getting f1 scores: 100%|██████████| 8/8 [00:04<00:00,  1.68it/s]\n",
            "Getting f1 scores: 100%|██████████| 20/20 [00:20<00:00,  1.00s/it]\n"
          ]
        }
      ],
      "source": [
        "# Get datasets\n",
        "train_base, val_base, test_base, test_novel, base_classes, novel_classes = get_dataset(do_preprocess=True)\n",
        "\n",
        "# Get loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_base, batch_size=64, shuffle=True, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_base, batch_size=64, shuffle=False, num_workers=2)\n",
        "test_base_loader = torch.utils.data.DataLoader(test_base, batch_size=128, shuffle=False, num_workers=2)\n",
        "test_novel_loader = torch.utils.data.DataLoader(test_novel, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "# Get f1 scores\n",
        "clip_model, _ = clip.load(CLIP_BACKBONE, device=DEVICE)\n",
        "f1_scores_val = get_f1_scores(clip_model, val_loader, base_classes, device=\"cuda:0\", label=\"Getting f1 scores\")\n",
        "f1_scores_test = get_f1_scores(clip_model, test_base_loader, base_classes, device=\"cuda:0\", label=\"Getting f1 scores\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryeM-5g6PY4w",
        "outputId": "209c450d-acac-46ca-d8e8-c1c7e6167fbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VAL\n",
            "hard-leaved pocket orchid accuracy: 0.00\n",
            "colt's foot accuracy: 0.00\n",
            "globe-flower accuracy: 0.00\n",
            "prince of wales feathers accuracy: 0.18\n",
            "love in the mist accuracy: 0.00\n",
            "mexican aster accuracy: 0.17\n",
            "cape flower accuracy: 0.00\n",
            "great masterwort accuracy: 0.00\n",
            "sword lily accuracy: 0.11\n",
            "bolero deep blue accuracy: 0.00\n",
            "TEST\n",
            "hard-leaved pocket orchid accuracy: 0.00\n",
            "colt's foot accuracy: 0.00\n",
            "globe-flower accuracy: 0.09\n",
            "prince of wales feathers accuracy: 0.00\n",
            "love in the mist accuracy: 0.00\n",
            "mexican aster accuracy: 0.17\n",
            "cape flower accuracy: 0.00\n",
            "great masterwort accuracy: 0.00\n",
            "sword lily accuracy: 0.05\n",
            "bolero deep blue accuracy: 0.00\n"
          ]
        }
      ],
      "source": [
        "common_thresh = 0.20 # Thresh to have same classes in val and test incorrectly predicted\n",
        "\n",
        "print(\"VAL\")\n",
        "for k in f1_scores_val:\n",
        "  if f1_scores_val[k] < common_thresh:\n",
        "    print(f\"{k} accuracy: {f1_scores_val[k]:.2f}\")\n",
        "print(\"TEST\")\n",
        "for k in f1_scores_test:\n",
        "  if f1_scores_test[k] < common_thresh:\n",
        "    print(f\"{k} accuracy: {f1_scores_test[k]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WMmCq8h0r0H"
      },
      "source": [
        "## Training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "cwiEh2lZuoBP"
      },
      "outputs": [],
      "source": [
        "def our_disef_eval(do_gen=True, do_train=True):\n",
        "  \"\"\"\"Evaluate DISEF technique.\n",
        "  Args:\n",
        "    do_gen (bool): generate synthetic samples or not.\n",
        "    do_train (bool): train model or load weights.\n",
        "  \"\"\"\n",
        "  # Get datasets\n",
        "  train_base, val_base, test_base, test_novel, base_classes, novel_classes = get_dataset(do_preprocess=True)\n",
        "\n",
        "  # Get loaders\n",
        "  train_loader = torch.utils.data.DataLoader(train_base, batch_size=TRAIN_BATCH, shuffle=True, num_workers=2)\n",
        "  val_loader = torch.utils.data.DataLoader(val_base, batch_size=VAL_BATCH, shuffle=False, num_workers=2)\n",
        "  test_base_loader = torch.utils.data.DataLoader(test_base, batch_size=TEST_BATCH, shuffle=False, num_workers=2)\n",
        "  test_novel_loader = torch.utils.data.DataLoader(test_novel, batch_size=TEST_BATCH, shuffle=False, num_workers=2)\n",
        "\n",
        "  # Get f1 scores\n",
        "  clip_model, _ = clip.load(CLIP_BACKBONE, device=DEVICE)\n",
        "  f1_scores = get_f1_scores(clip_model, val_loader, base_classes, device=\"cuda:0\", label=\"Getting f1 scores\")\n",
        "\n",
        "\n",
        "  if do_gen:\n",
        "    # Generate synthetic dataset\n",
        "    # generate_samples(f1_scores)\n",
        "    pass\n",
        "\n",
        "  syn_dataset = load_syn_dataset(do_preprocess=True)\n",
        "\n",
        "  # Get CLIP-LoRA\n",
        "  clip_model = get_clip_lora()\n",
        "\n",
        "  # Get syn loader\n",
        "  syn_loader = torch.utils.data.DataLoader(syn_dataset, batch_size=TRAIN_BATCH, shuffle=True, num_workers=2)\n",
        "\n",
        "  # Hyperparam\n",
        "  TRAIN_BATCH = 32 # From CLIP-LoRA paper\n",
        "  VAL_BATCH = 64\n",
        "  TEST_BATCH = 128\n",
        "  LR = 2e-4 # From CLIP-LoRA paper\n",
        "  NUM_EPOCHS = 15\n",
        "  PATIENCE = 3\n",
        "\n",
        "\n",
        "\n",
        "  # Get cost function\n",
        "  loss_fun = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  # Train\n",
        "  if do_train:\n",
        "    optimizer = optim.AdamW([p for p in clip_model.parameters() if p.requires_grad], lr=LR)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
        "\n",
        "    # Train\n",
        "    train_params = {\n",
        "        \"lora_model\": clip_model,\n",
        "        \"train_loader\": train_loader,\n",
        "        \"syn_loader\": syn_loader,\n",
        "        \"val_loader\": val_loader,\n",
        "        \"optimizer\": optimizer,\n",
        "        \"loss_fun\": loss_fun,\n",
        "        \"scheduler\": scheduler,\n",
        "        \"base_classes\": base_classes,\n",
        "        \"num_epochs\": NUM_EPOCHS,\n",
        "        \"patience\": PATIENCE,\n",
        "        \"device\": DEVICE\n",
        "    }\n",
        "    clip_model, log = lora_train(**train_params)\n",
        "    # Get training plot\n",
        "    plot_log(log)\n",
        "    # Save weights\n",
        "    save_lora(clip_model)\n",
        "  else:\n",
        "    load_lora(clip_model, device=DEVICE)\n",
        "\n",
        "  # Evaluate base and novel\n",
        "  base_accuracy = clip_test(clip_model, test_base_loader, base_classes, DEVICE, label=\"🧠 DISEF evaluation on Base\")\n",
        "  novel_accuracy = clip_test(clip_model, test_novel_loader, novel_classes, DEVICE, label=\"🧠 DISEF evaluation on Novel\")\n",
        "\n",
        "  # Show results\n",
        "  print() # For separating from progress bars\n",
        "  print(\"=\"*20,\"Evaluating DISEF\",\"=\"*20)\n",
        "  print(f\"🔍 Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
        "  print(f\"🔍 Novel classes accuracy: {novel_accuracy*100:.2f}%\")\n",
        "  print(f\"🔍 Harmonic Mean: {harmonic_mean(base_accuracy, novel_accuracy)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "9qFmb7Epu7fO"
      },
      "outputs": [],
      "source": [
        "if RUN_OUR_DISEF:\n",
        "  our_disef_eval(do_gen=True, do_train=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGo7uqx7nf4Y"
      },
      "source": [
        "The new CoCoOp results can now be compared with zero-shot:\n",
        "<div align=\"center\">\n",
        "\n",
        "| Model      | Base (↑) | Novel (↑) | Harmonic Mean |\n",
        "|------------|----------|-----------|---------------|\n",
        "| Zero-Shot  | 71.33%   | 78.24%    | 74.62%        |\n",
        "| CoCoOp     | 95,19%   | 71.11%    | 81.41%        |\n",
        "| CLIP-LoRA     | 96.81 %   | 74.13 %    | 83.96 %        |\n",
        "| DISEF     |  %   |  %    |  %        |            |\n",
        "| Our DISEF     |  %   |  %    |  %        |            |\n",
        "\n",
        "\n",
        "<div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U-RNoF4fQ1A"
      },
      "source": [
        "# References\n",
        "1. Radford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PmLR, 2021.\n",
        "2. Zhou, Kaiyang, et al. \"Conditional prompt learning for vision-language models.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n",
        "3. Zanella, Maxime, and Ismail Ben Ayed. \"Low-rank few-shot adaptation of vision-language models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.\n",
        "4. da Costa, Victor G. Turrisi, et al. \"Diversified in-domain synthesis with efficient fine-tuning for few-shot classification.\" arXiv preprint arXiv:2312.03046 (2023)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
